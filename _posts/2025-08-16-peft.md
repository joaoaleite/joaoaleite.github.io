---
layout: distill
title: (WIP) Parameter-Efficient Fine-Tuning (PEFT)
description: Notes on Parameter-Efficient Fine-Tuning (PEFT).
tags: study nlp
giscus_comments: false
date: 2025-08-16
featured: false

authors:
  - name: JoÃ£o Leite
    url: "https://jaleite.com"
    affiliations:
      name: University of Sheffield

bibliography: 2025-08-16-peft.bib

toc:
  - name: Introduction
    subsections:
      - name: Motivation
      - name: Core Idea

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

# Introduction

Modern Large Language Models (LLMs) are powerful general-purpose tools that need to be trained in multiple stages involving different methodologies. For the purpose of this post, we will discuss two of them.

The first is the foundational **pre-training** stage, where the model is trained from scratch (i.e., with randomly initialized weights) on a massive, diverse corpus of text. The objective is purely self-supervised learning, and the model typically learns to predict the next token in a given sentence. This step is computationally intensive, and its objective is to allow the model to learn general representations of language. At the end of this stage, the model is a powerful generalist but is not specialized for any specific task

The second stage is **fine-tuning**, where the base model (after pre-training), is trained using a much smaller, curated dataset for a specific task (e.g., becoming an assistant that follows instructions). The goal is to adapt the model for this task without it losing the general knowledge acquired during pre-training. 

In this context, **Parameter-Efficient Fine-Tuning (PEFT)**, as the name suggests, is a family of methods that allow us to perform the fine-tuning step in a more efficient and controlled manner.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Figure 1.</b> Pre-training and fine-tuning
    (<a href="https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf">Source</a>).
</div>

## Motivation

During pre-training, a model's weights transition from a random initialization to a state that holds general-purpose knowledge about language. The most straightforward method to adapt this model for a downstream task is to continue the training process from this checkpoint, updating every single weight using a new, task-specific dataset. This approach is known as **full fine-tuning (FFT)**.

However, full fine-tuning presents several significant drawbacks. First, by directly modifying the weights learned during pre-training, it risks erasing the valuable general-purpose knowledge they contain. This phenomenon is known as **catastrophic forgetting**, where the model's performance on general tasks degrades as it specializes.

Second, updating the entire model is computationally expensive. It requires a large amount of GPU memory to store not only the gradients for every parameter but also the memory-intensive states required by optimizers like AdamW.

Consider a 7-billion-parameter model using mixed-precision training:
* **Model Weights**: 7 billion parameters at 16-bit precision (2 bytes) require 14 GB of VRAM.
* **Gradients**: These are typically stored at the same precision, adding another 14 GB VRAM.
* **Optimizer States**: AdamW stores two states per parameter (momentum and variance), usually in 32-bit precision (4 bytes). This requires: 2 states * 7 billion parameters * 4 bytes = 56 GB VRAM.
* **Total**: 84GB VRAM

For reference, NVIDIA's latest consumer GPUs (as of August 2025) have up to 32GBs of VRAM (RTX 5090). Also, some recent LLMs have hundreds of billion of parameters rather than "only" 7 billion as in this example.

Another challenge with full fine-tuning is related to model storage and deployment. If an organization needs to adapt a base model for multiple, distinct downstream tasks (e.g., customer support, content summarization, and code generation), full fine-tuning requires creating and storing a complete, independent copy of the model for each task. Fine-tuning a 7B model for five different tasks would mean storing five separate 14 GB models, totaling 70 GB. This "model fork" approach is inefficient and complicates deployment and maintenance.

## Core Idea

The main idea behind PEFT is to **freeze the vast majority of the pre-trained model's parameters and only update a small, targeted subset** (often less than 1%). As a result, the memory and compute required for gradients and optimizer states is drastically reduced. This is achieved through several methods:

* **Reparameterization (e.g., LoRA)**: Train a small, separate component that represents an update to the frozen weights.

* **Additive (e.g., Adapters)**: Train new layers or "adapter" modules between the existing frozen layers of the model.

* **Selective (e.g., BitFit)**: Update only a tiny subset of the original model's parameters, such as the bias terms.


Regardless of the specific strategy, the principle of training only a fraction of the total parameters directly addresses the challenges of full fine-tuning. Since the original model's weights are frozen, the issue of catastrophic forgetting is largely mitigated. Also, by drastically reducing the number of trainable weights, the memory required for gradients and optimizer states shrinks, making it feasible to fine-tune larger models or use cheaper GPUs.

Let's revisit our previous example of fine-tuning a 7 billion parameter model now using a PEFT method that hypothetically reduces the number of trainable parameters to around 4 million, which is only 0.06% of the total 7 billion parameters. The vast majority of the model's weights remain frozen:

* **Frozen Model Weights**: The full 7 billion parameters still need to be loaded for the forward pass, requiring 14 GB of VRAM at 16-bit precision.

* **Trainable PEFT Weights**: 4 million parameters at 16-bit precision require 8 MB VRAM.

* **Gradients**: Gradients are only calculated for the trainable parameters, adding another 8 MB VRAM.

* **Optimizer States**: AdamW states for 4 million parameters require: 2 states * 4 million parameters * 4 bytes = 32 MB VRAM.

* **Total**: $\approx$ 14.05 GB VRAM. A reduction of around 83.2% in memory requirements compared to full fine-tuning.


Finally, instead of storing a complete model for each downstream task, you only need to save a single pre-trained model, and the small set of trained parameters for each downstream task. These lightweight modules, often just a few megabytes, can be loaded on top of the base model as needed. This is a far more scalable and efficient solution for managing multiple specialized models.

# PEFT Methods

## Low-Rank Adaptation (LoRA)
LoRA (Hu, Edward J., et al <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>) is one of the most popular and effective PEFT methods. Its core insight is that the change in a model's weights during fine-tuning ($\Delta W$) has a very low "intrinsic rank." In other words, the updates to the weight matrices don't need to be complex and full-rank; they can be efficiently approximated by much simpler, low-rank matrices.

> Reminder: The rank of a matrix is the number of linearly independent columns (or rows) it has. Intuitively, it measures the "dimensionality" of the information the matrix contains. A low-rank matrix is one where this information is highly redundant. For example, a 100x100 matrix might have a rank of only 5, meaning all 100 of its columns can be described as combinations of just 5 core "basis" columns.

Instead of updating the original pre-trained weight matrix $W$, LoRA represents the update $\Delta W$ as the product of two much smaller matrices $B$ and $A$.

$$\Delta W \approx B \cdot A$$

Where if the original weight matrix $W$ has dimensions $d \times k$, the new matrices will have dimensions $B$ $(d \times r)$ and $A$ $(r \times k)$. The hyperparameter $r$ is the rank of the decomposition, and it is much smaller than $d$ or $k$ (i.e., $r \ll min(d,k)$).

This decomposition is what leads to the massive parameter savings. For a large matrix, instead of training $d \times k$ parameters for the update, we only need to train $(d \times r) + (r \times k)$ parameters for the two smaller matrices.

> To give a concrete example, a single attention matrix in an LLM might have dimensions of $d{=}4096$ and $k{=}4096$.
* With FFT, we need to update the entire weight matrix $W$ $(d{\times}k{=}4096{\times}4096{=}16{,}777{,}216)$.
* With LoRA, chosing a small rank of $r=8$, we would instead train two matrices $B$ ($d{\times}r{=}4096{\times}8{=}32{,}768$) and $A$ ($r{\times}k{=}8{\times}4096{=}32{,}768$) , totalling  $65{,}536$ trainable parameters. A reduction of 99.6% from the FFT scenario.

The pre-trained weight matrix $W$ remains frozen during training, and only $B$ and $A$ are updated via backpropagation. At the end of training, the product of these learned matrices forms the final update ${\Delta}W{\leftarrow}B{\cdot}A$. This change can then be added to the original weights $W$ to produce the fine-tuned model: $W_{FT} = W_{pretrained} + \Delta W$.

During the forward pass, the model's output is calculated by adding the output of the frozen pre-trained layer to the output of the new LoRA path:

$$y=Wx + BAx = (W + BA)x$$

<div class="row mt-3" style="max-width: 50%; height: auto; margin: 0 auto;">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blogposts/2025-08-16-peft/lora_inference.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Figure 2.</b> LoRA: The input $x$ flows through the frozen layer $W$ and two smaller matrices $B$ and $A$.<br>
    (Source: Hu, Edward J., et al <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>).
</div>

### LoRA in the Transformer Architecture
LoRA is typically not applied to every single weight matrix in the Transformer architecture. The multi-head attention (MHA) block is the most effective place to apply LoRA, specifically to the dense layers that project the model's hidden state into the Query ($W_Q$) and Value ($W_V$) matrices.

The intuition for this choice is that adapting how the model queries for information (the $W_Q$ matrix) and how it extracts that information (the $W_V$ matrix) is a highly parameter-efficient way to teach it a new task. The original LoRA paper found that adapting these two matrices yielded the best performance for the number of parameters used. Nevertheless, it is possible to apply LoRA to the $W_K$ matrix, and the $W_O$ matrix (i.e., the final dense layer that combines the output of all individual attention heads).

For more complex fine-tuning tasks that may benefit from a higher number of trainable parameters, LoRA can also be applied to the linear layers in the point-wise feed-forward network (FFN) sub-layer.

### Implementation

The LoRA matrix $A$ is typically initialized with random Gaussian values, while $B$ is initialized to all zeros. This ensures that the initial update matrix $\Delta W{=}B{\cdot}A=0$, ensuring the starting behavior is identical to the original pre-trained model.

Also, an additional hyperparameter is introduced, ```lora_alpha``` ($\alpha$), which scales the magnitude of the update. This allows to control the adaptation strength independently of the rank $r$. A common practice is to set ```lora_alpha``` to twice the rank ```lora_alpha = 2 * r``` to amplify the update's effect without adding more parameters.

With this new hyperparameter, the forward pass becomes:

$$y=Wx+\frac{\alpha}{r}BAx$$

```python
class LoRALayer(nn.Module):
    def __init__(self, W: nn.Linear, r: int, alpha: float):
        super().__init__()

        self.r = r
        self.alpha = alpha
        
        device = W.weight.device
        d, k = (W.in_features, W.out_features)

        self.W = W
        self.lora_B = nn.Parameter(torch.zeros(d, r, device=device))
        self.lora_A = nn.Parameter(torch.randn(r, k, device=device))    

        self.merged = False  # flag to help merge weights after fine-tuning

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if not self.merged:
            frozen_out = self.W(x)

            lora_out = x @ self.lora_B @ self.lora_A
            lora_update = lora_out * (self.alpha / self.r)

            y = frozen_out + lora_update

            return y

        else:
            return self.W(x)

    ### These methods are used after fine-tuning ###
    @torch.no_grad()
    def merge(self):
        # Calculate the final update matrix delta_W
        delta_W = (self.alpha / self.r) * (self.lora_B @ self.lora_A)
        
        # Add the update to the original weights
        self.W.weight.data += delta_W.T # .T because linear.weight is (out, in)
        self.merged = True

    @torch.no_grad()
    def unmerge(self):
        if self.merged:
            delta_W = (self.alpha / self.r) * (self.lora_B @ self.lora_A)

            self.W.weight.data -= delta_W.T
            self.merged = False
```

## Adapters

## BitFit

# Helpers

TODO: helpers

```python
def get_peft_model(
  model: DecoderTransformer,
  r: int,
  alpha: float,
  target_modules: list[str]
):
    # Freeze all layers
    for parameter in model.parameters():
        parameter.requires_grad = False

    # Replace target modules with LoRALayers
    for name, module in model.named_modules():
        layer_name = name.split(".")[-1]

        if layer_name in target_modules:
            parent_layer_name = ".".join(name.split(".")[:-1])
            parent_module = model.get_submodule(parent_layer_name)

            if isinstance(module, nn.Linear):
                lora_layer = LoRALayer(module, r, alpha)
                setattr(parent_module, layer_name, lora_layer)

    # Unfreeze LoRA weights
    for name, param in model.named_parameters():
        if "lora_" in name:
            param.requires_grad = True

    return model
```

```python
def merge_and_unload(model):
    for name, module in model.named_modules():
        if isinstance(module, LoRALayer):
            module.merge()
            
            parent_name = ".".join(name.split('.')[:-1])
            parent_module = model.get_submodule(parent_name)
            layer_name = name.split('.')[-1]
            
            # Replace the layer with just the 'W' inside of it
            # (i.e., the merged weights)
            setattr(parent_module, layer_name, module.W)

    return model
```


# Why are PEFT methods not applied in the pre-training stage?