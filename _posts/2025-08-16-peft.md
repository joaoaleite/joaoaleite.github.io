---
layout: distill
title: (WIP) Parameter-Efficient Fine-Tuning (PEFT)
description: Notes on Parameter-Efficient Fine-Tuning (PEFT).
tags: study nlp
giscus_comments: false
date: 2025-08-16
featured: false

authors:
  - name: JoÃ£o Leite
    url: "https://jaleite.com"
    affiliations:
      name: University of Sheffield

bibliography: 2025-08-16-peft.bib

toc:
  - name: Introduction
    subsections:
      - name: Motivation
      - name: Core Idea

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

# Introduction

Modern Large Language Models (LLMs) are powerful general-purpose tools that need to be trained in multiple stages involving different methodologies. For the purpose of this post, we will discuss two of them.

The first is the foundational **pre-training** stage, where the model is trained from scratch (i.e., with randomly initialized weights) on a massive, diverse corpus of text. The objective is purely self-supervised learning, and the model typically learns to predict the next token in a given sentence. This step is computationally intensive, and its objective is to allow the model to learn general representations of language. At the end of this stage, the model is a powerful generalist but is not specialized for any specific task

The second stage is **fine-tuning**, where the base model (after pre-training), is trained using a much smaller, curated dataset for a specific task (e.g., becoming an assistant that follows instructions). The goal is to adapt the model for this task without it losing the general knowledge acquired during pre-training. 

In this context, **Parameter-Efficient Fine-Tuning (PEFT)**, as the name suggests, is a family of methods that allow us to perform the fine-tuning step in a more efficient and controlled manner.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Figure 1:</b> Pre-training and fine-tuning
    (<a href="https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf">Source</a>).
</div>

## Motivation

During pre-training, a model's weights transition from a random initialization to a state that holds general-purpose knowledge about language. The most straightforward method to adapt this model for a downstream task is to continue the training process from this checkpoint, updating every single weight using a new, task-specific dataset. This approach is known as **full fine-tuning**.

However, full fine-tuning presents several significant drawbacks. First, by directly modifying the weights learned during pre-training, it risks erasing the valuable general-purpose knowledge they contain. This phenomenon is known as **catastrophic forgetting**, where the model's performance on general tasks degrades as it specializes.

Second, updating the entire model is computationally expensive. It requires a large amount of GPU memory to store not only the gradients for every parameter but also the memory-intensive states required by optimizers like AdamW.

Consider a 7-billion-parameter model using mixed-precision training:
* **Model Weights**: 7 billion parameters at 16-bit precision (2 bytes) require 14 GB of VRAM.
* **Gradients**: These are typically stored at the same precision, adding another 14 GB VRAM.
* **Optimizer States**: AdamW stores two states per parameter (momentum and variance), usually in 32-bit precision (4 bytes). This requires: 2 parameters * 4 bytes * 7 billion weights = 56 GB VRAM.
* **Total**: 84GB VRAM

For reference, NVIDIA's latest consumer GPUs (as of August 2025) have up to 32GBs of VRAM (RTX 5090). Also, some recent LLMs have hundreds of billion of parameters rather than "only" 7 billion as in this example.

Another challenge with full fine-tuning is related to model storage and deployment. If an organization needs to adapt a base model for multiple, distinct downstream tasks (e.g., customer support, content summarization, and code generation), full fine-tuning requires creating and storing a complete, independent copy of the model for each task. Fine-tuning a 7B model for five different tasks would mean storing five separate 14 GB models, totaling 70 GB. This "model fork" approach is inefficient and complicates deployment and maintenance.

## Core Idea

The main idea behind PEFT is to **freeze the vast majority of the pre-trained model's parameters and only update a small, targeted subset** (often less than 1%). As a result, the memory and compute required for gradients and optimizer states is drastically reduced. This is achieved through several methods:

* **Reparameterization (e.g., LoRA)**: Train a small, separate component that represents an update to the frozen weights.

* **Additive (e.g., Adapters)**: Train new layers or "adapter" modules between the existing frozen layers of the model.

* **Selective (e.g., BitFit)**: Update only a tiny subset of the original model's parameters, such as the bias terms.


Regardless of the specific strategy, the principle of training only a fraction of the total parameters directly addresses the challenges of full fine-tuning. Since the original model's weights are frozen, the issue of catastrophic forgetting is largely mitigated. Also, by drastically reducing the number of trainable weights, the memory required for gradients and optimizer states shrinks, making it feasible to fine-tune larger models or use cheaper GPUs.

Let's revisit our previous example of fine-tuning a 7 billion parameter model now using a PEFT method like LoRA (Low-Rank Adaptation), which will be presented later. Let's assume a typical LoRA configuration where we adapt the query and value matrices in the attention mechanism with a rank of ```r=8```. In this scenario, the number of trainable parameters is approximately 4.2 million, which is only 0.06% of the total 7 billion parameters. The vast majority of the model's weights remain frozen:

* **Frozen Model Weights**: The full 7 billion parameters still need to be loaded for the forward pass, requiring 14 GB of VRAM at 16-bit precision.

* **Trainable PEFT Weights**: 4.2 million parameters at 16-bit precision require 8.4 MB VRAM.

* **Gradients**: Gradients are only calculated for the trainable parameters, adding another 8.4 MB VRAM.

* **Optimizer States**: AdamW states for 4.2 million parameters require: 2 parameters * 4 bytes * 4.2 million = 33.6 MB VRAM.

* **Total**: $\approx$ 14.05 GB VRAM. A reduction of around 83.2% in memory requirements compared to full fine-tuning.


Finally, instead of storing a complete model for each downstream task, you only need to save a single pre-trained model, and the small set of trained parameters for each downstream task. These lightweight modules, often just a few megabytes, can be loaded on top of the base model as needed. This is a far more scalable and efficient solution for managing multiple specialized models.

# PEFT Methods

## LoRA

## Adapters

## BitFit