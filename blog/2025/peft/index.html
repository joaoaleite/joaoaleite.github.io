<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> (WIP) Parameter-Efficient Fine-Tuning (PEFT) | João A. Leite </title> <meta name="author" content="João A. Leite"> <meta name="description" content="Notes on Parameter-Efficient Fine-Tuning (PEFT)."> <meta name="keywords" content="AI, Machine Learning, Natural Language Processing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A5%9B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joaoaleite.github.io/blog/2025/peft/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "(WIP) Parameter-Efficient Fine-Tuning (PEFT)",
            "description": "Notes on Parameter-Efficient Fine-Tuning (PEFT).",
            "published": "August 16, 2025",
            "authors": [
              
              {
                "author": "João Leite",
                "authorURL": "https://jaleite.com",
                "affiliations": [
                  {
                    "name": "University of Sheffield",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">João</span> A. Leite </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>(WIP) Parameter-Efficient Fine-Tuning (PEFT)</h1> <p>Notes on Parameter-Efficient Fine-Tuning (PEFT).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#core-idea">Core Idea</a> </li> </ul> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <p>Modern Large Language Models (LLMs) are powerful general-purpose tools that need to be trained in multiple stages involving different methodologies. For the purpose of this post, we will discuss two of them.</p> <p>The first is the foundational <strong>pre-training</strong> stage, where the model is trained from scratch (i.e., with randomly initialized weights) on a massive, diverse corpus of text. The objective is purely self-supervised learning, and the model typically learns to predict the next token in a given sentence. This step is computationally intensive, and its objective is to allow the model to learn general representations of language. At the end of this stage, the model is a powerful generalist but is not specialized for any specific task</p> <p>The second stage is <strong>fine-tuning</strong>, where the base model (after pre-training), is trained using a much smaller, curated dataset for a specific task (e.g., becoming an assistant that follows instructions). The goal is to adapt the model for this task without it losing the general knowledge acquired during pre-training.</p> <p>In this context, <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>, as the name suggests, is a family of methods that allow us to perform the fine-tuning step in a more efficient and controlled manner.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-480.webp 480w,/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-800.webp 800w,/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1:</b> Pre-training and fine-tuning (<a href="https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf" rel="external nofollow noopener" target="_blank">Source</a>). </div> <h2 id="motivation">Motivation</h2> <p>During pre-training, a model’s weights transition from a random initialization to a state that holds general-purpose knowledge about language. The most straightforward method to adapt this model for a downstream task is to continue the training process from this checkpoint, updating every single weight using a new, task-specific dataset. This approach is known as <strong>full fine-tuning</strong>.</p> <p>However, full fine-tuning presents several significant drawbacks. First, by directly modifying the weights learned during pre-training, it risks erasing the valuable general-purpose knowledge they contain. This phenomenon is known as <strong>catastrophic forgetting</strong>, where the model’s performance on general tasks degrades as it specializes.</p> <p>Second, updating the entire model is computationally expensive. It requires a large amount of GPU memory to store not only the gradients for every parameter but also the memory-intensive states required by optimizers like AdamW.</p> <p>Consider a 7-billion-parameter model using mixed-precision training:</p> <ul> <li> <strong>Model Weights</strong>: 7 billion parameters at 16-bit precision (2 bytes) require 14 GB of VRAM.</li> <li> <strong>Gradients</strong>: These are typically stored at the same precision, adding another 14 GB VRAM.</li> <li> <strong>Optimizer States</strong>: AdamW stores two states per parameter (momentum and variance), usually in 32-bit precision (4 bytes). This requires: 2 parameters * 4 bytes * 7 billion weights = 56 GB VRAM.</li> <li> <strong>Total</strong>: 84GB VRAM</li> </ul> <p>For reference, NVIDIA’s latest consumer GPUs (as of August 2025) have up to 32GBs of VRAM (RTX 5090). Also, some recent LLMs have hundreds of billion of parameters rather than “only” 7 billion as in this example.</p> <p>Another challenge with full fine-tuning is related to model storage and deployment. If an organization needs to adapt a base model for multiple, distinct downstream tasks (e.g., customer support, content summarization, and code generation), full fine-tuning requires creating and storing a complete, independent copy of the model for each task. Fine-tuning a 7B model for five different tasks would mean storing five separate 14 GB models, totaling 70 GB. This “model fork” approach is inefficient and complicates deployment and maintenance.</p> <h2 id="core-idea">Core Idea</h2> <p>The main idea behind PEFT is to <strong>freeze the vast majority of the pre-trained model’s parameters and only update a small, targeted subset</strong> (often less than 1%). As a result, the memory and compute required for gradients and optimizer states is drastically reduced. This is achieved through several methods:</p> <ul> <li> <p><strong>Reparameterization (e.g., LoRA)</strong>: Train a small, separate component that represents an update to the frozen weights.</p> </li> <li> <p><strong>Additive (e.g., Adapters)</strong>: Train new layers or “adapter” modules between the existing frozen layers of the model.</p> </li> <li> <p><strong>Selective (e.g., BitFit)</strong>: Update only a tiny subset of the original model’s parameters, such as the bias terms.</p> </li> </ul> <p>Regardless of the specific strategy, the principle of training only a fraction of the total parameters directly addresses the challenges of full fine-tuning. Since the original model’s weights are frozen, the issue of catastrophic forgetting is largely mitigated. Also, by drastically reducing the number of trainable weights, the memory required for gradients and optimizer states shrinks, making it feasible to fine-tune larger models or use cheaper GPUs.</p> <p>Let’s revisit our previous example of fine-tuning a 7 billion parameter model now using a PEFT method like LoRA (Low-Rank Adaptation), which will be presented later. Let’s assume a typical LoRA configuration where we adapt the query and value matrices in the attention mechanism with a rank of <code class="language-plaintext highlighter-rouge">r=8</code>. In this scenario, the number of trainable parameters is approximately 4.2 million, which is only 0.06% of the total 7 billion parameters. The vast majority of the model’s weights remain frozen:</p> <ul> <li> <p><strong>Frozen Model Weights</strong>: The full 7 billion parameters still need to be loaded for the forward pass, requiring 14 GB of VRAM at 16-bit precision.</p> </li> <li> <p><strong>Trainable PEFT Weights</strong>: 4.2 million parameters at 16-bit precision require 8.4 MB VRAM.</p> </li> <li> <p><strong>Gradients</strong>: Gradients are only calculated for the trainable parameters, adding another 8.4 MB VRAM.</p> </li> <li> <p><strong>Optimizer States</strong>: AdamW states for 4.2 million parameters require: 2 parameters * 4 bytes * 4.2 million = 33.6 MB VRAM.</p> </li> <li> <p><strong>Total</strong>: $\approx$ 14.05 GB VRAM. A reduction of around 83.2% in memory requirements compared to full fine-tuning.</p> </li> </ul> <p>Finally, instead of storing a complete model for each downstream task, you only need to save a single pre-trained model, and the small set of trained parameters for each downstream task. These lightweight modules, often just a few megabytes, can be loaded on top of the base model as needed. This is a far more scalable and efficient solution for managing multiple specialized models.</p> <h1 id="peft-methods">PEFT Methods</h1> <h2 id="lora">LoRA</h2> <h2 id="adapters">Adapters</h2> <h2 id="bitfit">BitFit</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-08-16-peft.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 João A. Leite. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-news",title:"news",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-wip-parameter-efficient-fine-tuning-peft",title:"(WIP) Parameter-Efficient Fine-Tuning (PEFT)",description:"Notes on Parameter-Efficient Fine-Tuning (PEFT).",section:"Posts",handler:()=>{window.location.href="/blog/2025/peft/"}},{id:"post-transformers",title:"Transformers",description:"Notes on the Transformer architecture.",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformers/"}},{id:"news-my-aacl-paper-quot-toxic-language-detection-in-social-media-for-brazilian-portuguese-new-dataset-and-multilingual-analysis-quot-https-aclanthology-org-2020-aacl-main-91-was-awarded-best-undergraduate-paper-by-the-computer-science-department-at-universidade-federal-de-s\xe3o-carlos",title:"My AACL paper [&quot;Toxic language detection in social media for Brazilian Portuguese: New...",description:"",section:"News"},{id:"news-i-started-my-phd-in-computer-science-at-the-university-of-sheffield-sponsored-by-an-epsrc-doctoral-training-partnership-dtp-scholarship",title:"I started my PhD in Computer Science at the University of Sheffield sponsored...",description:"",section:"News"},{id:"news-presenting-my-paper-quot-noisy-self-training-with-data-augmentations-for-offensive-and-hate-speech-detection-tasks-quot-https-aclanthology-org-2023-ranlp-1-68-at-ranlp-2023-in-varna-bulgaria",title:"Presenting my paper [&quot;Noisy Self-Training with Data Augmentations for Offensive and Hate Speech...",description:"",section:"News"},{id:"news-i-was-hired-as-a-part-time-research-associate-in-nlp-and-machine-learning-at-the-university-of-sheffield",title:"I was hired as a part-time Research Associate in NLP and Machine Learning...",description:"",section:"News"},{id:"news-attending-the-lisbon-machine-learning-school-lxmls-http-lxmls-it-pt-2024-sponsored-by-a-google-and-zendesk-scholarship",title:"Attending the Lisbon Machine Learning School ([LxMLS](http://lxmls.it.pt/2024/)) sponsored by a Google and ZenDesk...",description:"",section:"News"},{id:"news-my-paper-quot-euvsdisinfo-a-dataset-for-multilingual-detection-of-pro-kremlin-disinformation-in-news-articles-https-dl-acm-org-doi-abs-10-1145-3627673-3679167-quot-has-been-accepted-for-presentation-at-cikm-2024-in-boise-usa",title:"My paper &quot;[EUvsDisinfo: A Dataset for Multilingual Detection of Pro-Kremlin Disinformation in News...",description:"",section:"News"},{id:"news-my-paper-quot-a-cross-domain-study-of-the-use-of-persuasion-techniques-in-online-disinformation-https-arxiv-org-abs-2412-15098-quot-has-been-accepted-for-presentation-at-www-2025-in-sydney-australia",title:"My paper &quot;[A Cross-Domain Study of the Use of Persuasion Techniques in Online...",description:"",section:"News"},{id:"news-leading-an-in-house-research-hackathon-project-alongside-four-invited-researchers",title:"Leading an in-house research hackathon project alongside four invited researchers.",description:"",section:"News"},{id:"news-my-paper-quot-weakly-supervised-veracity-classification-with-llm-predicted-credibility-signals-https-epjds-epj-org-articles-epjdata-abs-2025-01-13688-2025-article-534-13688-2025-article-534-html-quot-has-been-published-in-the-epj-data-science-journal",title:"My paper &quot;[Weakly supervised veracity classification with LLM-predicted credibility signals](https://epjds.epj.org/articles/epjdata/abs/2025/01/13688_2025_Article_534/13688_2025_Article_534.html)&quot; has been published...",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A.%6C%65%69%74%65@%73%68%65%66%66%69%65%6C%64.%61%63.%75%6B","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-3587-853X","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=-OhzTN4AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/joaoaleite","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/joao-augusto-leite","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://www.sheffield.ac.uk/cs/people/research-staff/joao-leite","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>