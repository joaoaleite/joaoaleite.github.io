<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> (WIP) Parameter-Efficient Fine-Tuning (PEFT) | João A. Leite </title> <meta name="author" content="João A. Leite"> <meta name="description" content="Notes on Parameter-Efficient Fine-Tuning (PEFT)."> <meta name="keywords" content="AI, Machine Learning, Natural Language Processing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A5%9B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joaoaleite.github.io/blog/2025/peft/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "(WIP) Parameter-Efficient Fine-Tuning (PEFT)",
            "description": "Notes on Parameter-Efficient Fine-Tuning (PEFT).",
            "published": "August 16, 2025",
            "authors": [
              
              {
                "author": "João Leite",
                "authorURL": "https://jaleite.com",
                "affiliations": [
                  {
                    "name": "University of Sheffield",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">João</span> A. Leite </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>(WIP) Parameter-Efficient Fine-Tuning (PEFT)</h1> <p>Notes on Parameter-Efficient Fine-Tuning (PEFT).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#core-idea">Core Idea</a> </li> </ul> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <p>Modern Large Language Models (LLMs) are powerful general-purpose tools that need to be trained in multiple stages involving different methodologies. For the purpose of this post, we will discuss two of them.</p> <p>The first is the foundational <strong>pre-training</strong> stage, where the model is trained from scratch (i.e., with randomly initialized weights) on a massive, diverse corpus of text. The objective is purely self-supervised learning, and the model typically learns to predict the next token in a given sentence. This step is computationally intensive, and its objective is to allow the model to learn general representations of language. At the end of this stage, the model is a powerful generalist but is not specialized for any specific task</p> <p>The second stage is <strong>fine-tuning</strong>, where the base model (after pre-training), is trained using a much smaller, curated dataset for a specific task (e.g., becoming an assistant that follows instructions). The goal is to adapt the model for this task without it losing the general knowledge acquired during pre-training.</p> <p>In this context, <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>, as the name suggests, is a family of methods that allow us to perform the fine-tuning step in a more efficient and controlled manner.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-480.webp 480w,/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-800.webp 800w,/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blogposts/2025-08-16-peft/pretraining-and-finetuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Pre-training and fine-tuning (<a href="https://medium.com/@eordaxd/fine-tuning-vs-pre-training-651d05186faf" rel="external nofollow noopener" target="_blank">Source</a>). </div> <h2 id="motivation">Motivation</h2> <p>During pre-training, a model’s weights transition from a random initialization to a state that holds general-purpose knowledge about language. The most straightforward method to adapt this model for a downstream task is to continue the training process from this checkpoint, updating every single weight using a new, task-specific dataset. This approach is known as <strong>full fine-tuning (FFT)</strong>.</p> <p>However, full fine-tuning presents several significant drawbacks. First, by directly modifying the weights learned during pre-training, it risks erasing the valuable general-purpose knowledge they contain. This phenomenon is known as <strong>catastrophic forgetting</strong>, where the model’s performance on general tasks degrades as it specializes.</p> <p>Second, updating the entire model is computationally expensive. It requires a large amount of GPU memory to store not only the gradients for every parameter but also the memory-intensive states required by optimizers like AdamW.</p> <p>Consider a 7-billion-parameter model using mixed-precision training:</p> <ul> <li> <strong>Model Weights</strong>: 7 billion parameters at 16-bit precision (2 bytes) require 14 GB of VRAM.</li> <li> <strong>Gradients</strong>: These are typically stored at the same precision, adding another 14 GB VRAM.</li> <li> <strong>Optimizer States</strong>: AdamW stores two states per parameter (momentum and variance), usually in 32-bit precision (4 bytes). This requires: 2 states * 7 billion parameters * 4 bytes = 56 GB VRAM.</li> <li> <strong>Total</strong>: 84GB VRAM</li> </ul> <p>For reference, NVIDIA’s latest consumer GPUs (as of August 2025) have up to 32GBs of VRAM (RTX 5090). Also, some recent LLMs have hundreds of billion of parameters rather than “only” 7 billion as in this example.</p> <p>Another challenge with full fine-tuning is related to model storage and deployment. If an organization needs to adapt a base model for multiple, distinct downstream tasks (e.g., customer support, content summarization, and code generation), full fine-tuning requires creating and storing a complete, independent copy of the model for each task. Fine-tuning a 7B model for five different tasks would mean storing five separate 14 GB models, totaling 70 GB. This “model fork” approach is inefficient and complicates deployment and maintenance.</p> <h2 id="core-idea">Core Idea</h2> <p>The main idea behind PEFT is to <strong>freeze the vast majority of the pre-trained model’s parameters and only update a small, targeted subset</strong> (often less than 1%). As a result, the memory and compute required for gradients and optimizer states is drastically reduced. This is achieved through several methods:</p> <ul> <li> <p><strong>Reparameterization (e.g., LoRA)</strong>: Train a small, separate component that represents an update to the frozen weights.</p> </li> <li> <p><strong>Additive (e.g., Adapters)</strong>: Train new layers or “adapter” modules between the existing frozen layers of the model.</p> </li> <li> <p><strong>Selective (e.g., BitFit)</strong>: Update only a tiny subset of the original model’s parameters, such as the bias terms.</p> </li> </ul> <p>Regardless of the specific strategy, the principle of training only a fraction of the total parameters directly addresses the challenges of full fine-tuning. Since the original model’s weights are frozen, the issue of catastrophic forgetting is largely mitigated. Also, by drastically reducing the number of trainable weights, the memory required for gradients and optimizer states shrinks, making it feasible to fine-tune larger models or use cheaper GPUs.</p> <p>Let’s revisit our previous example of fine-tuning a 7 billion parameter model now using a PEFT method that hypothetically reduces the number of trainable parameters to around 4 million, which is only 0.06% of the total 7 billion parameters. The vast majority of the model’s weights remain frozen:</p> <ul> <li> <p><strong>Frozen Model Weights</strong>: The full 7 billion parameters still need to be loaded for the forward pass, requiring 14 GB of VRAM at 16-bit precision.</p> </li> <li> <p><strong>Trainable PEFT Weights</strong>: 4 million parameters at 16-bit precision require 8 MB VRAM.</p> </li> <li> <p><strong>Gradients</strong>: Gradients are only calculated for the trainable parameters, adding another 8 MB VRAM.</p> </li> <li> <p><strong>Optimizer States</strong>: AdamW states for 4 million parameters require: 2 states * 4 million parameters * 4 bytes = 32 MB VRAM.</p> </li> <li> <p><strong>Total</strong>: $\approx$ 14.05 GB VRAM. A reduction of around 83.2% in memory requirements compared to full fine-tuning.</p> </li> </ul> <p>Finally, instead of storing a complete model for each downstream task, you only need to save a single pre-trained model, and the small set of trained parameters for each downstream task. These lightweight modules, often just a few megabytes, can be loaded on top of the base model as needed. This is a far more scalable and efficient solution for managing multiple specialized models.</p> <h1 id="peft-methods">PEFT Methods</h1> <h2 id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</h2> <p>LoRA (Hu, Edward J., et al <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>) is one of the most popular and effective PEFT methods. Its core insight is that the change in a model’s weights during fine-tuning ($\Delta W$) has a very low “intrinsic rank.” In other words, the updates to the weight matrices don’t need to be complex and full-rank; they can be efficiently approximated by much simpler, low-rank matrices.</p> <blockquote> <p>Reminder: The rank of a matrix is the number of linearly independent columns (or rows) it has. Intuitively, it measures the “dimensionality” of the information the matrix contains. A low-rank matrix is one where this information is highly redundant. For example, a 100x100 matrix might have a rank of only 5, meaning all 100 of its columns can be described as combinations of just 5 core “basis” columns.</p> </blockquote> <p>Instead of updating the original pre-trained weight matrix $W$, LoRA represents the update $\Delta W$ as the product of two much smaller matrices $B$ and $A$.</p> \[\Delta W \approx B \cdot A\] <p>Where if the original weight matrix $W$ has dimensions $d \times k$, the new matrices will have dimensions $B$ $(d \times r)$ and $A$ $(r \times k)$. The hyperparameter $r$ is the rank of the decomposition, and it is much smaller than $d$ or $k$ (i.e., $r \ll min(d,k)$).</p> <p>This decomposition is what leads to the massive parameter savings. For a large matrix, instead of training $d \times k$ parameters for the update, we only need to train $(d \times r) + (r \times k)$ parameters for the two smaller matrices.</p> <blockquote> <p>To give a concrete example, a single attention matrix in an LLM might have dimensions of $d{=}4096$ and $k{=}4096$.</p> <ul> <li>With FFT, we need to update the entire weight matrix $W$ $(d{\times}k{=}4096{\times}4096{=}16{,}777{,}216)$.</li> <li>With LoRA, chosing a small rank of $r=8$, we would instead train two matrices $B$ ($d{\times}r{=}4096{\times}8{=}32{,}768$) and $A$ ($r{\times}k{=}8{\times}4096{=}32{,}768$) , totalling $65{,}536$ trainable parameters. A reduction of 99.6% from the FFT scenario.</li> </ul> </blockquote> <p>The pre-trained weight matrix $W$ remains frozen during training, and only $B$ and $A$ are updated via backpropagation. At the end of training, the product of these learned matrices forms the final update ${\Delta}W{\leftarrow}B{\cdot}A$. This change can then be added to the original weights $W$ to produce the fine-tuned model: $W_{FT} = W_{pretrained} + \Delta W$.</p> <p>During the forward pass, the model’s output is calculated by adding the output of the frozen pre-trained layer to the output of the new LoRA path:</p> \[y=Wx + BAx = (W + BA)x\] <div class="row mt-3" style="max-width: 50%; height: auto; margin: 0 auto;"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogposts/2025-08-16-peft/lora_inference-480.webp 480w,/assets/img/blogposts/2025-08-16-peft/lora_inference-800.webp 800w,/assets/img/blogposts/2025-08-16-peft/lora_inference-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/blogposts/2025-08-16-peft/lora_inference.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 2.</b> LoRA: The input $x$ flows through the frozen layer $W$ and two smaller matrices $B$ and $A$.<br> (Source: Hu, Edward J., et al <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>). </div> <h3 id="lora-in-the-transformer-architecture">LoRA in the Transformer Architecture</h3> <p>LoRA is typically not applied to every single weight matrix in the Transformer architecture. The multi-head attention (MHA) block is the most effective place to apply LoRA, specifically to the dense layers that project the model’s hidden state into the Query ($W_Q$) and Value ($W_V$) matrices.</p> <p>The intuition for this choice is that adapting how the model queries for information (the $W_Q$ matrix) and how it extracts that information (the $W_V$ matrix) is a highly parameter-efficient way to teach it a new task. The original LoRA paper found that adapting these two matrices yielded the best performance for the number of parameters used. Nevertheless, it is possible to apply LoRA to the $W_K$ matrix, and the $W_O$ matrix (i.e., the final dense layer that combines the output of all individual attention heads).</p> <p>For more complex fine-tuning tasks that may benefit from a higher number of trainable parameters, LoRA can also be applied to the linear layers in the point-wise feed-forward network (FFN) sub-layer.</p> <h3 id="implementation">Implementation</h3> <p>The LoRA matrix $A$ is typically initialized with random Gaussian values, while $B$ is initialized to all zeros. This ensures that the initial update matrix $\Delta W{=}B{\cdot}A=0$, ensuring the starting behavior is identical to the original pre-trained model.</p> <p>Also, an additional hyperparameter is introduced, <code class="language-plaintext highlighter-rouge">lora_alpha</code> ($\alpha$), which scales the magnitude of the update. This allows to control the adaptation strength independently of the rank $r$. A common practice is to set <code class="language-plaintext highlighter-rouge">lora_alpha</code> to twice the rank <code class="language-plaintext highlighter-rouge">lora_alpha = 2 * r</code> to amplify the update’s effect without adding more parameters.</p> <p>With this new hyperparameter, the forward pass becomes:</p> \[y=Wx+\frac{\alpha}{r}BAx\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        
        <span class="n">device</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">device</span>
        <span class="n">d</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">W</span><span class="p">.</span><span class="n">out_features</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>    

        <span class="n">self</span><span class="p">.</span><span class="n">merged</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># flag to help merge weights after fine-tuning
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">frozen_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">lora_out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span>
            <span class="n">lora_update</span> <span class="o">=</span> <span class="n">lora_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">r</span><span class="p">)</span>

            <span class="n">y</span> <span class="o">=</span> <span class="n">frozen_out</span> <span class="o">+</span> <span class="n">lora_update</span>

            <span class="k">return</span> <span class="n">y</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nc">W</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1">### These methods are used after fine-tuning ###
</span>    <span class="nd">@torch.no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Calculate the final update matrix delta_W
</span>        <span class="n">delta_W</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">)</span>
        
        <span class="c1"># Add the update to the original weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">delta_W</span><span class="p">.</span><span class="n">T</span> <span class="c1"># .T because linear.weight is (out, in)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">merged</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="nd">@torch.no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">unmerge</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">merged</span><span class="p">:</span>
            <span class="n">delta_W</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">delta_W</span><span class="p">.</span><span class="n">T</span>
            <span class="n">self</span><span class="p">.</span><span class="n">merged</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <h2 id="adapters">Adapters</h2> <h2 id="bitfit">BitFit</h2> <h1 id="helpers">Helpers</h1> <p>TODO: helpers</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_peft_model</span><span class="p">(</span>
  <span class="n">model</span><span class="p">:</span> <span class="n">DecoderTransformer</span><span class="p">,</span>
  <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
  <span class="n">target_modules</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">):</span>
    <span class="c1"># Freeze all layers
</span>    <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">parameter</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="c1"># Replace target modules with LoRALayers
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
        <span class="n">layer_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="n">target_modules</span><span class="p">:</span>
            <span class="n">parent_layer_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">parent_module</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_submodule</span><span class="p">(</span><span class="n">parent_layer_name</span><span class="p">)</span>

            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">lora_layer</span> <span class="o">=</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
                <span class="nf">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">lora_layer</span><span class="p">)</span>

    <span class="c1"># Unfreeze LoRA weights
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">lora_</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">merge_and_unload</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">LoRALayer</span><span class="p">):</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">merge</span><span class="p">()</span>
            
            <span class="n">parent_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">parent_module</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_submodule</span><span class="p">(</span><span class="n">parent_name</span><span class="p">)</span>
            <span class="n">layer_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># Replace the layer with just the 'W' inside of it
</span>            <span class="c1"># (i.e., the merged weights)
</span>            <span class="nf">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <h1 id="why-are-peft-methods-not-applied-in-the-pre-training-stage">Why are PEFT methods not applied in the pre-training stage?</h1> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-08-16-peft.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 João A. Leite. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-news",title:"news",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-wip-parameter-efficient-fine-tuning-peft",title:"(WIP) Parameter-Efficient Fine-Tuning (PEFT)",description:"Notes on Parameter-Efficient Fine-Tuning (PEFT).",section:"Posts",handler:()=>{window.location.href="/blog/2025/peft/"}},{id:"post-transformers",title:"Transformers",description:"Notes on the Transformer architecture.",section:"Posts",handler:()=>{window.location.href="/blog/2024/transformers/"}},{id:"news-my-aacl-paper-quot-toxic-language-detection-in-social-media-for-brazilian-portuguese-new-dataset-and-multilingual-analysis-quot-https-aclanthology-org-2020-aacl-main-91-was-awarded-best-undergraduate-paper-by-the-computer-science-department-at-universidade-federal-de-s\xe3o-carlos",title:"My AACL paper [&quot;Toxic language detection in social media for Brazilian Portuguese: New...",description:"",section:"News"},{id:"news-i-started-my-phd-in-computer-science-at-the-university-of-sheffield-sponsored-by-an-epsrc-doctoral-training-partnership-dtp-scholarship",title:"I started my PhD in Computer Science at the University of Sheffield sponsored...",description:"",section:"News"},{id:"news-presenting-my-paper-quot-noisy-self-training-with-data-augmentations-for-offensive-and-hate-speech-detection-tasks-quot-https-aclanthology-org-2023-ranlp-1-68-at-ranlp-2023-in-varna-bulgaria",title:"Presenting my paper [&quot;Noisy Self-Training with Data Augmentations for Offensive and Hate Speech...",description:"",section:"News"},{id:"news-i-was-hired-as-a-part-time-research-associate-in-nlp-and-machine-learning-at-the-university-of-sheffield",title:"I was hired as a part-time Research Associate in NLP and Machine Learning...",description:"",section:"News"},{id:"news-attending-the-lisbon-machine-learning-school-lxmls-http-lxmls-it-pt-2024-sponsored-by-a-google-and-zendesk-scholarship",title:"Attending the Lisbon Machine Learning School ([LxMLS](http://lxmls.it.pt/2024/)) sponsored by a Google and ZenDesk...",description:"",section:"News"},{id:"news-my-paper-quot-euvsdisinfo-a-dataset-for-multilingual-detection-of-pro-kremlin-disinformation-in-news-articles-https-dl-acm-org-doi-abs-10-1145-3627673-3679167-quot-has-been-accepted-for-presentation-at-cikm-2024-in-boise-usa",title:"My paper &quot;[EUvsDisinfo: A Dataset for Multilingual Detection of Pro-Kremlin Disinformation in News...",description:"",section:"News"},{id:"news-my-paper-quot-a-cross-domain-study-of-the-use-of-persuasion-techniques-in-online-disinformation-https-arxiv-org-abs-2412-15098-quot-has-been-accepted-for-presentation-at-www-2025-in-sydney-australia",title:"My paper &quot;[A Cross-Domain Study of the Use of Persuasion Techniques in Online...",description:"",section:"News"},{id:"news-leading-an-in-house-research-hackathon-project-alongside-four-invited-researchers",title:"Leading an in-house research hackathon project alongside four invited researchers.",description:"",section:"News"},{id:"news-my-paper-quot-weakly-supervised-veracity-classification-with-llm-predicted-credibility-signals-https-epjds-epj-org-articles-epjdata-abs-2025-01-13688-2025-article-534-13688-2025-article-534-html-quot-has-been-published-in-the-epj-data-science-journal",title:"My paper &quot;[Weakly supervised veracity classification with LLM-predicted credibility signals](https://epjds.epj.org/articles/epjdata/abs/2025/01/13688_2025_Article_534/13688_2025_Article_534.html)&quot; has been published...",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A.%6C%65%69%74%65@%73%68%65%66%66%69%65%6C%64.%61%63.%75%6B","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-3587-853X","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=-OhzTN4AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/joaoaleite","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/joao-augusto-leite","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://www.sheffield.ac.uk/cs/people/research-staff/joao-leite","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>