@article{xie_deep_learning_semantic,
author = {Xie, Huiqiang and Qin, Zhijin and Li, Geoffrey and Juang, Biing-Hwang},
year = {2021},
month = {04},
pages = {1-1},
title = {Deep Learning Enabled Semantic Communication Systems},
volume = {PP},
journal = {IEEE Transactions on Signal Processing},
doi = {10.1109/TSP.2021.3071210}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179/",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734"
}

@article{raj-2019-analysis,
author = {Sathia Raj, Dinesh and Vijayakumar, Vignesh and Rawal, Bharat and Yang, Longzhi},
year = {2019},
month = {09},
pages = {},
title = {Analysis of Big Data Technology for Health Care Services},
doi = {10.48550/arXiv.1909.03029}
}

@inproceedings{chung-etal-2014-empirical,
title = "Empirical evaluation of gated recurrent neural networks on sequence modeling",
author = "Junyoung Chung and Caglar Gulcehre and Kyunghyun Cho and Yoshua Bengio",
year = "2014",
language = "English (US)",
booktitle = "NIPS 2014 Workshop on Deep Learning, December 2014",
}

@article{allam-mcewen-2023-paying,
    author = {Allam, Tarek, Jr. and McEwen, Jason D},
    title = {Paying attention to astronomical transients: introducing the time-series transformer for photometric classification},
    journal = {RAS Techniques and Instruments},
    volume = {3},
    number = {1},
    pages = {209-223},
    year = {2023},
    month = {10},
    abstract = {Future surveys such as the Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will observe an order of magnitude more astrophysical transient events than any previous survey before. With this deluge of photometric data, it will be impossible for all such events to be classified by humans alone. Recent efforts have sought to leverage machine learning methods to tackle the challenge of astronomical transient classification, with ever improving success. Transformers are a recently developed deep learning architecture, first proposed for natural language processing, that have shown a great deal of recent success. In this work, we develop a new transformer architecture, which uses multihead self-attention at its core, for general multivariate time-series data. Furthermore, the proposed time-series transformer architecture supports the inclusion of an arbitrary number of additional features, while also offering interpretability. We apply the time-series transformer to the task of photometric classification, minimizing the reliance of expert domain knowledge for feature selection, while achieving results comparable with state-of-the-art photometric classification methods. We achieve a logarithmic-loss of 0.507 on imbalanced data in a representative setting using data from the Photometric LSST Astronomical Time-Series Classification Challenge. Moreover, we achieve a micro-averaged receiver-operating-characteristic area under curve of 0.98 and micro-averaged precision–recall area under curve of 0.87.},
    issn = {2752-8200},
    doi = {10.1093/rasti/rzad046},
    url = {https://doi.org/10.1093/rasti/rzad046},
    eprint = {https://academic.oup.com/rasti/article-pdf/3/1/209/61224576/rzad046.pdf},
}

@article{mikolov-2013-efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{garg-ramakrishnan-2020-advances,
  title={Advances in quantum deep learning: An overview},
  author={Garg, Siddhant and Ramakrishnan, Goutham},
  journal={arXiv preprint arXiv:2005.04316},
  year={2020}
}

@InProceedings{he-2016-deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}